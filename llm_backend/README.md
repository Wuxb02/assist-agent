# LLM Backend ğŸ”§

AssistGen æ™ºèƒ½å®¢æœç³»ç»Ÿçš„åç«¯æœåŠ¡æ ¸å¿ƒæ¨¡å—

## ğŸ“Š æŠ€æœ¯æ ˆ

![FastAPI](https://img.shields.io/badge/FastAPI-005571?logo=fastapi&logoColor=white)
![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python&logoColor=white)
![LangGraph](https://img.shields.io/badge/LangGraph-Multi--Agent-green)
![GraphRAG](https://img.shields.io/badge/GraphRAG-Microsoft-orange)
![Neo4j](https://img.shields.io/badge/Neo4j-Knowledge%20Graph-brightgreen?logo=neo4j)

## ğŸ—ï¸ æ¶æ„è®¾è®¡

```mermaid
flowchart TB
    subgraph "ğŸŒ API Gateway"
        A[FastAPI Server<br/>main.py]
    end
    
    subgraph "ğŸ”§ Core Services"
        B[LLM Factory<br/>llm_factory.py]
        C[Conversation Service<br/>conversation_service.py]
        D[Search Service<br/>search_service.py]
        E[Indexing Service<br/>indexing_service.py]
    end
    
    subgraph "ğŸ¤– AI Agents"
        F[LangGraph Router<br/>lg_agent/main.py]
        G[Knowledge Graph Tools<br/>kg_sub_graph/]
        H[Multi-Tool Workflow<br/>multi_tools.py]
    end
    
    subgraph "ğŸ“Š Data Layer"
        I[(MySQL<br/>Conversations)]
        J[(Neo4j<br/>Knowledge Graph)]
        K[(Redis<br/>Cache)]
        L[GraphRAG<br/>Vector Store]
    end
    
    A --> B
    A --> C
    A --> D
    A --> E
    A --> F
    
    B --> I
    C --> I
    F --> G
    F --> H
    G --> J
    E --> K
    E --> L
```

## ğŸš€ å¿«é€Ÿå¯åŠ¨

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿æ‚¨åœ¨é¡¹ç›®æ ¹ç›®å½• (`deepseek_agent/`)ï¼š

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows

# å®‰è£…ä¾èµ–ï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
pip install -r requirements.txt
```

### 2. é…ç½®æ–‡ä»¶

åˆ›å»ºå¹¶é…ç½® `.env` æ–‡ä»¶ï¼š

```bash
cp .env.example .env
```

å…³é”®é…ç½®é¡¹ï¼š

```bash
# ğŸ¤– AI æœåŠ¡é…ç½®
CHAT_SERVICE=deepseek          # èŠå¤©æœåŠ¡: deepseek/ollama
REASON_SERVICE=deepseek        # æ¨ç†æœåŠ¡: deepseek/ollama  
AGENT_SERVICE=deepseek         # AgentæœåŠ¡: deepseek/ollama

# ğŸ”‘ API å¯†é’¥
DEEPSEEK_API_KEY=sk-xxxxx      # DeepSeek APIå¯†é’¥
SERPAPI_KEY=xxxxx              # SerpAPIæœç´¢å¯†é’¥
VISION_API_KEY=xxxxx           # è§†è§‰æ¨¡å‹å¯†é’¥

# ğŸ—„ï¸ æ•°æ®åº“è¿æ¥
MYSQL_HOST=localhost
MYSQL_PORT=3306
MYSQL_USER=root
MYSQL_PASSWORD=your_password
MYSQL_DATABASE=assistgen

NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_password

REDIS_HOST=localhost
REDIS_PORT=6379
```

### 3. å¯åŠ¨æœåŠ¡

```bash
# è¿›å…¥åç«¯ç›®å½•
cd llm_backend

# æ–¹å¼1: ç›´æ¥å¯åŠ¨
python run.py

# æ–¹å¼2: ä½¿ç”¨ uvicorn
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

ğŸ‰ æœåŠ¡å¯åŠ¨åè®¿é—®ï¼š
- **APIæ–‡æ¡£**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **å¥åº·æ£€æŸ¥**: http://localhost:8000/health

## ğŸ“š API æ¥å£

### æ ¸å¿ƒç«¯ç‚¹

| ç«¯ç‚¹ | æ–¹æ³• | æè¿° | ç¤ºä¾‹ |
|------|------|------|------|
| `/chat` | POST | ğŸ’¬ èŠå¤©å¯¹è¯ | æ™ºèƒ½å®¢æœå¯¹è¯ |
| `/reason` | POST | ğŸ§  æ¨ç†åˆ†æ | å¤æ‚é—®é¢˜æ¨ç† |
| `/upload` | POST | ğŸ“¤ æ–‡ä»¶ä¸Šä¼  | RAGæ–‡æ¡£ä¸Šä¼  |
| `/langgraph/query` | POST | ğŸ¤– AgentæŸ¥è¯¢ | å¤šæ™ºèƒ½ä½“ä»»åŠ¡ |
| `/conversation/{id}` | GET | ğŸ“‹ å¯¹è¯å†å² | è·å–èŠå¤©è®°å½• |
| `/conversation/` | GET | ğŸ“Š å¯¹è¯åˆ—è¡¨ | åˆ†é¡µè·å–å¯¹è¯ |

### è¯·æ±‚ç¤ºä¾‹

#### ğŸ’¬ èŠå¤©æ¥å£

```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£è¿™ä¸ªäº§å“",
    "conversation_id": "uuid-string",
    "user_id": "user123"
  }'
```

#### ğŸ¤– AgentæŸ¥è¯¢

```bash
curl -X POST "http://localhost:8000/langgraph/query" \
  -H "Content-Type: application/json" \
  -d '{
    "conversation_id": "uuid-string",
    "query": "å¸®æˆ‘åˆ†æå½“å‰å¸‚åœºè¶‹åŠ¿",
    "thread_id": "thread-uuid"
  }'
```

#### ğŸ“¤ æ–‡ä»¶ä¸Šä¼ 

```bash
curl -X POST "http://localhost:8000/upload" \
  -F "files=@document.pdf" \
  -F "user_uuid=user123"
```

## ğŸ› ï¸ å¼€å‘æŒ‡å—

### é¡¹ç›®ç»“æ„è¯¦è§£

```
llm_backend/
â”œâ”€â”€ ğŸš€ main.py                    # FastAPIåº”ç”¨å…¥å£
â”œâ”€â”€ â–¶ï¸ run.py                     # æœåŠ¡å¯åŠ¨è„šæœ¬
â”œâ”€â”€ ğŸ“ app/
â”‚   â”œâ”€â”€ ğŸŒ api/                   # APIè·¯ç”±æ¨¡å—
â”‚   â”‚   â””â”€â”€ auth.py               # è®¤è¯ç›¸å…³æ¥å£
â”‚   â”œâ”€â”€ âš™ï¸ core/                  # æ ¸å¿ƒé…ç½®
â”‚   â”‚   â”œâ”€â”€ config.py             # ç¯å¢ƒé…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ database.py           # æ•°æ®åº“è¿æ¥
â”‚   â”‚   â”œâ”€â”€ logger.py             # æ—¥å¿—é…ç½®
â”‚   â”‚   â”œâ”€â”€ middleware.py         # ä¸­é—´ä»¶
â”‚   â”‚   â””â”€â”€ security.py           # å®‰å…¨é…ç½®
â”‚   â”œâ”€â”€ ğŸ“Š graphrag/              # GraphRAGé›†æˆ
â”‚   â”‚   â”œâ”€â”€ graphrag/             # æ ¸å¿ƒGraphRAGæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ dev/                  # å¼€å‘å·¥å…·
â”‚   â”‚   â””â”€â”€ origin_data/          # åŸå§‹æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ ğŸ¤– lg_agent/              # LangGraphæ™ºèƒ½ä½“
â”‚   â”‚   â”œâ”€â”€ main.py               # Agentå…¥å£
â”‚   â”‚   â”œâ”€â”€ lg_states.py          # çŠ¶æ€å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ lg_prompts.py         # æç¤ºè¯æ¨¡æ¿
â”‚   â”‚   â””â”€â”€ kg_sub_graph/         # çŸ¥è¯†å›¾è°±å­å›¾
â”‚   â”œâ”€â”€ ğŸ—„ï¸ models/                # æ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ chat.py               # èŠå¤©æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ conversation.py       # å¯¹è¯æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ message.py            # æ¶ˆæ¯æ¨¡å‹
â”‚   â”‚   â””â”€â”€ user.py               # ç”¨æˆ·æ¨¡å‹
â”‚   â”œâ”€â”€ ğŸ’¬ prompts/               # æç¤ºè¯æ¨¡æ¿
â”‚   â”‚   â””â”€â”€ search_prompts.py     # æœç´¢æç¤ºè¯
â”‚   â”œâ”€â”€ ğŸ”§ services/              # ä¸šåŠ¡æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ llm_factory.py        # LLMæœåŠ¡å·¥å‚
â”‚   â”‚   â”œâ”€â”€ deepseek_service.py   # DeepSeekæœåŠ¡
â”‚   â”‚   â”œâ”€â”€ ollama_service.py     # OllamaæœåŠ¡
â”‚   â”‚   â”œâ”€â”€ search_service.py     # æœç´¢æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ conversation_service.py # å¯¹è¯æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ indexing_service.py   # ç´¢å¼•æœåŠ¡
â”‚   â”‚   â””â”€â”€ embedding_service.py  # å‘é‡åµŒå…¥æœåŠ¡
â”‚   â”œâ”€â”€ ğŸ§ª test/                  # æµ‹è¯•æ–‡ä»¶
â”‚   â”‚   â”œâ”€â”€ ollama_benchmark.py   # æ€§èƒ½åŸºå‡†æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ deepseek_sync.py      # DeepSeekåŒæ­¥æµ‹è¯•
â”‚   â”‚   â””â”€â”€ test_fastapi.py       # FastAPIæ¥å£æµ‹è¯•
â”‚   â””â”€â”€ ğŸ› ï¸ tools/                 # å·¥å…·æ¨¡å—
â”‚       â”œâ”€â”€ definitions.py        # å·¥å…·å®šä¹‰
â”‚       â””â”€â”€ search.py             # æœç´¢å·¥å…·
â””â”€â”€ ğŸ“ scripts/                   # å·¥å…·è„šæœ¬
    â””â”€â”€ init_db.py                # æ•°æ®åº“åˆå§‹åŒ–
```

### æ·»åŠ æ–°åŠŸèƒ½

#### 1. ğŸ†• æ·»åŠ æ–°çš„APIç«¯ç‚¹

```python
# app/api/new_feature.py
from fastapi import APIRouter

router = APIRouter()

@router.post("/new-feature")
async def new_feature_endpoint():
    return {"message": "New feature"}
```

åœ¨ `main.py` ä¸­æ³¨å†Œï¼š

```python
from app.api.new_feature import router as new_feature_router
app.include_router(new_feature_router, prefix="/api/v1")
```

#### 2. ğŸ”§ æ·»åŠ æ–°çš„æœåŠ¡

```python
# app/services/new_service.py
from app.core.logger import get_logger

logger = get_logger(service="new_service")

class NewService:
    def __init__(self):
        self.initialized = True
        
    async def process_data(self, data):
        logger.info("Processing data...")
        return processed_data
```

#### 3. ğŸ¤– æ‰©å±•AgentåŠŸèƒ½

åœ¨ `lg_agent/` ç›®å½•ä¸‹æ·»åŠ æ–°çš„çŠ¶æ€å’Œå·¥å…·ï¼š

```python
# lg_agent/new_agent_tools.py
from langchain.tools import tool

@tool
def new_agent_tool(query: str) -> str:
    """æ–°çš„Agentå·¥å…·"""
    return f"Processed: {query}"
```

## ğŸ§ª æµ‹è¯•ä¸è°ƒè¯•

### æ€§èƒ½æµ‹è¯•

```bash
# è¿›å…¥æµ‹è¯•ç›®å½•
cd app/test

# Ollamaæ¨¡å‹æ€§èƒ½æµ‹è¯•
python ollama_benchmark.py

# DeepSeekåŒæ­¥è°ƒç”¨æµ‹è¯•
python deepseek_sync.py

# DeepSeekæµå¼è°ƒç”¨æµ‹è¯•  
python deepseek_stream.py

# FastAPIæ¥å£æµ‹è¯•
python test_fastapi.py
```

### è°ƒè¯•æ¨¡å¼

å¯åŠ¨å¼€å‘æœåŠ¡å™¨ï¼ˆè‡ªåŠ¨é‡è½½ï¼‰ï¼š

```bash
python run.py
# æˆ–è€…
uvicorn main:app --reload --log-level debug
```

### æ—¥å¿—æŸ¥çœ‹

æ—¥å¿—é…ç½®åœ¨ `app/core/logger.py`ï¼Œæ”¯æŒç»“æ„åŒ–æ—¥å¿—ï¼š

```python
from app.core.logger import get_logger, log_structured

logger = get_logger(service="my_service")
logger.info("Service started")

# ç»“æ„åŒ–æ—¥å¿—
log_structured("user_action", {
    "user_id": "123",
    "action": "chat",
    "timestamp": datetime.now()
})
```

## ğŸ”’ å®‰å…¨é…ç½®

### APIè®¤è¯

å½“å‰æ”¯æŒåŸºç¡€è®¤è¯ï¼Œå¯åœ¨ `app/api/auth.py` ä¸­æ‰©å±•ï¼š

```python
from app.core.security import verify_token

@app.middleware("http")
async def auth_middleware(request: Request, call_next):
    # å®ç°JWTæˆ–å…¶ä»–è®¤è¯é€»è¾‘
    pass
```

### ç¯å¢ƒå˜é‡

æ•æ„Ÿä¿¡æ¯é€šè¿‡ç¯å¢ƒå˜é‡ç®¡ç†ï¼Œå‚è€ƒ `app/core/config.py`

## ğŸš€ éƒ¨ç½²å»ºè®®

### ç”Ÿäº§ç¯å¢ƒé…ç½®

```bash
# ç”Ÿäº§ç¯å¢ƒå¯åŠ¨
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4

# ä½¿ç”¨Gunicorn + Uvicorn
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker
```

### Docker éƒ¨ç½²

```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY llm_backend/ .
EXPOSE 8000

CMD ["python", "run.py"]
```

### ç›‘æ§ä¸æ—¥å¿—

- ä½¿ç”¨ `loguru` è¿›è¡Œç»“æ„åŒ–æ—¥å¿—è®°å½•
- é›†æˆ Prometheus metricsï¼ˆå¯é€‰ï¼‰
- å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼š`/health`

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. **ä»£ç è§„èŒƒ**: éµå¾ª PEP 8 ç¼–ç è§„èŒƒ
2. **æäº¤æ ¼å¼**: ä½¿ç”¨è¯­ä¹‰åŒ–æäº¤ä¿¡æ¯
3. **æµ‹è¯•è¦†ç›–**: æ–°åŠŸèƒ½éœ€è¦æ·»åŠ æµ‹è¯•ç”¨ä¾‹
4. **æ–‡æ¡£æ›´æ–°**: æ›´æ–°ç›¸å…³APIæ–‡æ¡£


---

<div align="center">

**ğŸ”§ LLM Backend - AssistGen æ ¸å¿ƒæœåŠ¡**

*åŸºäº FastAPI + LangGraph + GraphRAG æ„å»º*

</div>